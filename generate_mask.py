from cProfile import label
import os
import time
import argparse
from xml.etree.ElementInclude import include
import numpy as np
from collections import OrderedDict
from pydantic import IntEnumError
import torch
from torch.utils.data import DataLoader, RandomSampler
from torchvision.datasets import CIFAR10
import torchvision.transforms as transforms

import models
import data.poison_cifar as poison

parser = argparse.ArgumentParser(description='Train poisoned networks')

# Basic model parameters. You can not change
parser.add_argument('--batch-size',
                    type=int,
                    default=128,
                    help='the batch size for dataloader')
parser.add_argument('--lr',
                    type=float,
                    default=0.2,
                    help='the learning rate for mask optimization')
parser.add_argument('--nb-iter',
                    type=int,
                    default=1000,
                    help='the number of iterations for training')
parser.add_argument('--print-every',
                    type=int,
                    default=250,
                    help='print results every few iterations')
parser.add_argument('--data-dir',
                    type=str,
                    default='./data',
                    help='dir to the dataset')
parser.add_argument('--val-frac',
                    type=float,
                    default=0.01,
                    help='The fraction of the validate set')
parser.add_argument('--output-dir', type=str, default='mask_out/')
parser.add_argument('--poison-target',
                    type=int,
                    default=0,
                    help='target class of backdoor attack')
parser.add_argument('--trigger-alpha',
                    type=float,
                    default=1.0,
                    help='the transparency of the trigger pattern.')
parser.add_argument('--anp-steps', type=int, default=2)
parser.add_argument('--anp-eps', type=float, default=0.1)

# Parameters you can change
parser.add_argument('--anp-alpha', type=float, default=0.2)

args = parser.parse_args()
args_dict = vars(args)
print(args_dict)
os.makedirs(args.output_dir, exist_ok=True)
device = 'cuda' if torch.cuda.is_available() else 'cpu'


def main():
	MEAN_CIFAR10 = (0.4914, 0.4822, 0.4465)
	STD_CIFAR10 = (0.2023, 0.1994, 0.2010)
	transform_train = transforms.Compose([
	    transforms.RandomCrop(32, padding=4),
	    transforms.RandomHorizontalFlip(),
	    transforms.ToTensor(),
	    transforms.Normalize(MEAN_CIFAR10, STD_CIFAR10)
	])
	transform_test = transforms.Compose([
	    transforms.ToTensor(),
	    transforms.Normalize(MEAN_CIFAR10, STD_CIFAR10)
	])

	# Step 1: create dataset - clean val set, poisoned test set, and clean test set.
	trigger_info = torch.load('./trigger_info_foranp.th', map_location=device)

	orig_train = CIFAR10(root=args.data_dir,
	                     train=True,
	                     download=True,
	                     transform=transform_train)
	_, clean_val = poison.split_dataset(dataset=orig_train,
	                                    val_frac=args.val_frac,
	                                    perm=np.loadtxt(
	                                        './data/cifar_shuffle.txt',
	                                        dtype=int))
	clean_test = CIFAR10(root=args.data_dir,
	                     train=False,
	                     download=True,
	                     transform=transform_test)
	poison_test = poison.add_predefined_trigger_cifar(
	    data_set=clean_test, trigger_info=trigger_info)

	random_sampler = RandomSampler(data_source=clean_val,
	                               replacement=True,
	                               num_samples=args.print_every *
	                               args.batch_size)
	clean_val_loader = DataLoader(clean_val,
	                              batch_size=args.batch_size,
	                              shuffle=False,
	                              sampler=random_sampler,
	                              num_workers=0)
	poison_test_loader = DataLoader(poison_test,
	                                batch_size=args.batch_size,
	                                num_workers=0)
	clean_test_loader = DataLoader(clean_test,
	                               batch_size=args.batch_size,
	                               num_workers=0)

	# Step 2: load model checkpoints and trigger info
	checkpoint = "./badnetsmodel_foranp.th"
	state_dict = torch.load(checkpoint, map_location=device)
	net = getattr(models, 'resnet18')(num_classes=10,
	                                  norm_layer=models.NoisyBatchNorm2d)
	load_state_dict(net, orig_state_dict=state_dict)
	net = net.to(device)
	criterion = torch.nn.CrossEntropyLoss().to(device)

	parameters = list(net.named_parameters())
	mask_params = [v for n, v in parameters if "neuron_mask" in n]
	mask_optimizer = torch.optim.SGD(mask_params, lr=args.lr, momentum=0.9)
	noise_params = [v for n, v in parameters if "neuron_noise" in n]
	noise_optimizer = torch.optim.SGD(noise_params,
	                                  lr=args.anp_eps / args.anp_steps)

	# Step 3: train backdoored models
	print(
	    'Iter \t lr \t Time \t TrainLoss \t TrainACC \t PoisonLoss \t PoisonACC \t CleanLoss \t CleanACC'
	)
	nb_repeat = int(np.ceil(args.nb_iter / args.print_every))
	for i in range(nb_repeat):
		start = time.time()
		lr = mask_optimizer.param_groups[0]['lr']
		train_loss, train_acc = mask_train(model=net,
		                                   criterion=criterion,
		                                   data_loader=clean_val_loader,
		                                   mask_opt=mask_optimizer,
		                                   noise_opt=noise_optimizer)
		cl_test_loss, cl_test_acc = test(model=net,
		                                 criterion=criterion,
		                                 data_loader=clean_test_loader)
		po_test_loss, po_test_acc = test(model=net,
		                                 criterion=criterion,
		                                 data_loader=poison_test_loader)
		end = time.time()
		print(
		    '{} \t {:.3f} \t {:.1f} \t {:.4f} \t {:.4f} \t {:.4f} \t {:.4f} \t {:.4f} \t {:.4f}'
		    .format((i + 1) * args.print_every, lr, end - start, train_loss,
		            train_acc, po_test_loss, po_test_acc, cl_test_loss,
		            cl_test_acc))
	save_mask_scores(net.state_dict(),
	                 os.path.join(args.output_dir, 'mask_values.txt'))


def load_state_dict(net, orig_state_dict):
	if 'state_dict' in orig_state_dict.keys():
		orig_state_dict = orig_state_dict['state_dict']
	if "state_dict" in orig_state_dict.keys():
		orig_state_dict = orig_state_dict["state_dict"]

	new_state_dict = OrderedDict()
	for k, v in net.state_dict().items():
		if k in orig_state_dict.keys():
			new_state_dict[k] = orig_state_dict[k]
		elif 'running_mean_noisy' in k or 'running_var_noisy' in k or 'num_batches_tracked_noisy' in k:
			new_state_dict[k] = orig_state_dict[k[:-6]].clone().detach()
		else:
			new_state_dict[k] = v
	net.load_state_dict(new_state_dict)


def clip_mask(model, lower=0.0, upper=1.0):
	#### clip to the mask parameter
	params = [
	    param for name, param in model.named_parameters()
	    if 'neuron_mask' in name
	]
	with torch.no_grad():
		for param in params:
			param.clamp_(lower, upper)


def sign_grad(model):
	#### get the sign gradients to the noise
	noise = [
	    param for name, param in model.named_parameters()
	    if 'neuron_noise' in name
	]
	for p in noise:
		p.grad.data = torch.sign(p.grad.data)


def include_noise(model):
	#### weights become (m+\delta)*weights, bias become (1+\ksi)*b
	for name, module in model.named_modules():
		if isinstance(module, models.NoisyBatchNorm2d) or isinstance(
		    module, models.NoisyBatchNorm1d):
			module.include_noise()


def exclude_noise(model):
	#### weights become (m)*weights, bias become (1)*b
	for name, module in model.named_modules():
		if isinstance(module, models.NoisyBatchNorm2d) or isinstance(
		    module, models.NoisyBatchNorm1d):
			module.exclude_noise()


def reset(model, rand_init):
	#### random init perturbations
	for name, module in model.named_modules():
		if isinstance(module, models.NoisyBatchNorm2d) or isinstance(
		    module, models.NoisyBatchNorm1d):
			module.reset(rand_init=rand_init, eps=args.anp_eps)


def mask_train(model, criterion, mask_opt, noise_opt, data_loader):
	'''
    model: input model
    criterion: loss function
    mask_opt: optimizer to optimize mask
    noise_opt: optimzier to optimize noise
    data_loader: dataloader for a subset of clean images 
    args.anp_alpha: hyperparameter to balancing the natural loss and perturbed loss, see PPT
    args.anp_eps  : maximum pertubation budget for noise 
    args.anp_steps: iteration numbers for searching noise (inner maximization)
    '''
	model.train()
	total_correct = 0
	total_loss = 0.0
	nb_samples = 0
	for i, (images, labels) in enumerate(data_loader):
		images, labels = images.to(device), labels.to(device)
		nb_samples += images.size(0)
		### Write your code here to optimize mask
		# step 1: calculate the adversarial perturbation for neurons
		# step 2: calculate noise loss
		# step 3: calculate clean loss
		# step 4: ANP loss and update mask

		# Retrieve noise and mask parameters from optimizers
		noise_params = noise_opt.param_groups[0]['params']
		mask_params = mask_opt.param_groups[0]['params']

		# === Step 1: Calculate the adversarial perturbation for neurons (PGD) ===
		# Initialize noise (e.g., random start for PGD within [-args.anp_eps, args.anp_eps])
		reset(model, rand_init=True)

		# Set requires_grad for noise and mask params appropriately for inner loop
		for p_n in noise_params:
			p_n.requires_grad = True
		for p_m in mask_params:
			p_m.requires_grad = False  # Mask is fixed during inner noise optimization

		# Inner loop to find adversarial noise
		for _ in range(args.anp_steps):
			noise_opt.zero_grad()

			include_noise(model)  # Incorporate current noise into model
			output_adv_step = model(images)
			loss_adv_maximize = -criterion(output_adv_step, labels)

			# Maximize loss_adv_maximize w.r.t. noise parameters using PGD ascent
			# For SGD optimizer (which minimizes), use (-loss) to achieve ascent.
			# Gradient becomes -dL/d(noise).
			loss_adv_maximize.backward()

			# sign_grad(model) changes p_noise.grad.data to sign(-dL/d(noise))
			sign_grad(model)

			# noise_opt.step() performs: noise = noise - lr * sign(-dL/d(noise))
			# which is: noise = noise + lr * sign(dL/d(noise)) (PGD ascent step)
			noise_opt.step()

			# Project noise back to the epsilon ball [-args.anp_eps, args.anp_eps]
			with torch.no_grad():
				for p_n in noise_params:
					p_n.data.clamp_(-args.anp_eps, args.anp_eps)

		# Adversarial noise is now found.
		# Detach noise from graph for mask update step by setting requires_grad = False.
		for p_n in noise_params:
			p_n.requires_grad = False
		# Ensure mask parameters are trainable for the mask update step.
		for p_m in mask_params:
			p_m.requires_grad = True

		# === Step 2: Calculate loss with adversarial noise (L_perturbed) ===
		include_noise(model)  # Model uses the found adversarial noise
		output_perturbed = model(images)
		loss_perturbed = criterion(output_perturbed, labels)

		# === Step 3: Calculate clean loss (L_natural) ===
		exclude_noise(model)  # Model uses original mask, no noise
		output_clean = model(images)
		loss_clean = criterion(output_clean, labels)

		# For accuracy calculation (based on clean data predictions)
		with torch.no_grad():
			pred = output_clean.data.max(1)[1]
			total_correct += pred.eq(labels.data.view_as(pred)).sum().item()

		# === Step 4: ANP loss and update mask ===
		# L_ANP = L_natural + anp_alpha * L_perturbed
		anp_loss = args.anp_alpha * loss_clean + (
		    1 - args.anp_alpha) * loss_perturbed

		total_loss += anp_loss.item()  # Accumulate for average loss over epoch

		# Update mask
		mask_opt.zero_grad()
		anp_loss.backward()  # Calculate gradients for mask parameters
		mask_opt.step()  # Update mask
		clip_mask(model)  # Clip mask values (e.g., to [0, 1])
	loss = total_loss / len(data_loader)
	acc = float(total_correct) / nb_samples
	return loss, acc


def test(model, criterion, data_loader):
	model.eval()
	total_correct = 0
	total_loss = 0.0
	with torch.no_grad():
		for i, (images, labels) in enumerate(data_loader):
			images, labels = images.to(device), labels.type(
			    torch.LongTensor).to(device)
			output = model(images)
			if len(labels.shape) == 2:
				labels = labels.squeeze(1)
			total_loss += criterion(output, labels).item()
			pred = output.data.max(1)[1]
			total_correct += pred.eq(labels.data.view_as(pred)).sum()
	loss = total_loss / len(data_loader)
	acc = float(total_correct) / len(data_loader.dataset)
	return loss, acc


def save_mask_scores(state_dict, file_name):
	mask_values = []
	count = 0
	for name, param in state_dict.items():
		if 'neuron_mask' in name:
			for idx in range(param.size(0)):
				neuron_name = '.'.join(name.split('.')[:-1])
				mask_values.append('{} \t {} \t {} \t {:.4f} \n'.format(
				    count, neuron_name, idx, param[idx].item()))
				count += 1
	with open(file_name, "w") as f:
		f.write('No \t Layer Name \t Neuron Idx \t Mask Score \n')
		f.writelines(mask_values)


if __name__ == '__main__':
	main()
